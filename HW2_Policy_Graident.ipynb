{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will solve a classic control problem - CartPole using policy gradient methods.\n",
    "\n",
    "First, you will implement the \"vanilla\" policy gradient method, i.e., a method that repeatedly computes **unbiased** estimates $\\hat{g}$ of $\\nabla_{\\theta} E[\\sum_t r_t]$ and takes gradient ascent steps $\\theta \\rightarrow \\theta + \\epsilon \\hat{g}$ so as to increase the total rewards collected in each episode. To make sure our code can solve multiple MDPs with different policy parameterizations, provided code follows an OOP manner and represents MDP and Policy as classes.\n",
    "\n",
    "The following code constructs an instance of the MDP using OpenAI gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2016-10-15 16:22:57,934] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from policy_gradient import util\n",
    "from policy_gradient.policy import CategoricalPolicy\n",
    "from policy_gradient.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "import collections\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# CartPole-v0 is a MDP with finite state and action space. \n",
    "# In this environment, A pendulum is attached by an un-actuated joint to a cart, \n",
    "# and the goal is to prevent it from falling over. You can apply a force of +1 or -1 to the cart.\n",
    "# A reward of +1 is provided for every timestep that the pendulum remains upright. \n",
    "# To visualize CartPole-v0, please see https://gym.openai.com/envs/CartPole-v0\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: construct a neural network to represent policy\n",
    "\n",
    "Make sure you know how to construct neural network using tensorflow.\n",
    "\n",
    "1. Open **homework2/policy_gradient/policy.py**.\n",
    "2. Follow the instruction of Problem 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: compute the surrogate loss\n",
    "\n",
    "If there are $N$ episodes in an iteration, then for $i$ th episode we define $R_t^i = \\sum_{{t^′}=t}^T \\gamma^{{t^′}-t}r(s_{t^′}, a_{t^′})$ as the accumulated discounted rewards from timestep $t$ to the end of that episode, where $\\gamma$ is the discount rate.\n",
    "\n",
    "The pseudocode for the REINFORCE algorithm is as below:\n",
    "\n",
    "1. Initialize policy $\\pi$ with parameter $\\theta_1$.\n",
    "2. For iteration $k = 1, 2, ...$:\n",
    "    * Sample N episodes $\\tau_1, \\tau_2, ..., \\tau_N$ under the current policy $\\theta_k$, where $\\tau_i =(s_i^t,a_i^t,R_i^t)_{t=0}^{T−1}$. Note that the last state is dropped since no action is taken after observing the last state.\n",
    "    * Compute the empirical policy gradient using formula: $$\\hat{g} = E_{\\pi_\\theta}[\\nabla_{\\theta} log\\pi_\\theta(a_t^i | s_t^i) R_t^i]$$\n",
    "    * Take a gradient step: $\\theta_{k+1} = \\theta_k + \\epsilon \\hat{g}$.\n",
    "    \n",
    "    \n",
    "Note that we can transform the policy gradient formula as\n",
    "\n",
    "$$\\hat{g} = \\nabla_{\\theta} \\frac{1}{(NT)}(\\sum_{i=1}^N \\sum_{t=0}^T log\\pi_\\theta(a_t^i | s_t^i) R_t^i)$$\n",
    "\n",
    "and $L(\\theta) = \\frac{1}{(NT)}(\\sum_{i=1}^N \\sum_{t=0}^T log\\pi_\\theta(a_t^i | s_t^i) R_t^i)$ is called the surrogate loss. \n",
    "\n",
    "We can first construct the computation graph for $L(\\theta)$, and then take its gradient as the empirical policy gradient.\n",
    "\n",
    "\n",
    "1. Open **homework2/policy_gradient/policy.py**.\n",
    "2. Follow the instruction of Problem 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# Construct a neural network to represent policy which maps observed state to action. \n",
    "in_dim = util.flatten_space(env.observation_space)\n",
    "out_dim = util.flatten_space(env.action_space)\n",
    "hidden_dim = 8\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "policy = CategoricalPolicy(in_dim, out_dim, hidden_dim, opt, sess)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Implement a function that computes the accumulated discounted rewards of each timestep _t_ from _t_ to the end of the episode.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "rewards = [1, 1, 1]\n",
    "discount_rate = 0.99\n",
    "util.discount_cumsum(rewards, discount_rate)\n",
    "```\n",
    "\n",
    "should return:\n",
    "\n",
    "`array([ 2.9701,  1.99  ,  1.    ])`\n",
    "\n",
    "1. Open **homework/policy_gradient/util.py**.\n",
    "2. Implement the commented function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Use baseline to reduce the variance of our gradient estimate.\n",
    "\n",
    "1. Fill in the function `process_paths` of class `PolicyOptimizer` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    def __init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "        discount_rate=.99):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.baseline = baseline\n",
    "        self.env = env\n",
    "        self.n_iter = n_iter\n",
    "        self.n_episode = n_episode\n",
    "        self.path_length = path_length\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "    def sample_path(self):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = self.env.reset()\n",
    "\n",
    "        for _ in range(self.path_length):\n",
    "            a = self.policy.act(ob.reshape(1, -1))\n",
    "            next_ob, r, done, _ = self.env.step(a)\n",
    "            obs.append(ob)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            ob = next_ob\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return dict(\n",
    "            observations=np.array(obs),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "        )\n",
    "\n",
    "    def process_paths(self, paths):\n",
    "        for p in paths:\n",
    "            if self.baseline != None:\n",
    "                b = self.baseline.predict(p)\n",
    "            else:\n",
    "                b = 0\n",
    "            \n",
    "            # `p[\"rewards\"]` is a matrix contains the rewards of each timestep in a sample path\n",
    "            r = util.discount_cumsum(p[\"rewards\"], self.discount_rate)\n",
    "\n",
    "            \"\"\"\n",
    "            Problem 4:\n",
    "\n",
    "            1. Variable `b` is the reward predicted by our baseline\n",
    "            2. Use it to reduce variance and then assign the result to the variable `a`\n",
    "\n",
    "            Sample solution should be only 1 line.\n",
    "            \"\"\"\n",
    "            # YOUR CODE HERE >>>>>>\n",
    "            a = r-b\n",
    "            # <<<<<<<<\n",
    "\n",
    "            p[\"returns\"] = r\n",
    "            p[\"baselines\"] = b\n",
    "            p[\"advantages\"] = (a - a.mean()) / (a.std() + 1e-8) # normalize\n",
    "\n",
    "        obs = np.concatenate([ p[\"observations\"] for p in paths ])\n",
    "        actions = np.concatenate([ p[\"actions\"] for p in paths ])\n",
    "        rewards = np.concatenate([ p[\"rewards\"] for p in paths ])\n",
    "        advantages = np.concatenate([ p[\"advantages\"] for p in paths ])\n",
    "\n",
    "        return dict(\n",
    "            observations=obs,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            advantages=advantages,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            paths = []\n",
    "            for _ in range(self.n_episode):\n",
    "                paths.append(self.sample_path())\n",
    "            data = self.process_paths(paths)\n",
    "            loss = self.policy.train(data[\"observations\"], data[\"actions\"], data[\"advantages\"])\n",
    "            #print 'avg_rewards = ',[sum(p[\"rewards\"]) for p in paths] \n",
    "            avg_return = np.mean([sum(p[\"rewards\"]) for p in paths])\n",
    "            print(\"Iteration {}: Average Return = {}\".format(i, avg_return))\n",
    "            \n",
    "            # CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials.\n",
    "            if avg_return >= 195:\n",
    "                print(\"Solve at {} iterations, which equals {} episodes.\".format(i, i*100))\n",
    "                break\n",
    "\n",
    "            if self.baseline != None:\n",
    "                self.baseline.fit(paths)\n",
    "            iteration_to_complete = i\n",
    "        return iteration_to_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Average Return = 26.57\n",
      "Iteration 2: Average Return = 28.59\n",
      "Iteration 3: Average Return = 30.09\n",
      "Iteration 4: Average Return = 29.49\n",
      "Iteration 5: Average Return = 32.86\n",
      "Iteration 6: Average Return = 32.16\n",
      "Iteration 7: Average Return = 33.83\n",
      "Iteration 8: Average Return = 35.85\n",
      "Iteration 9: Average Return = 37.46\n",
      "Iteration 10: Average Return = 37.42\n",
      "Iteration 11: Average Return = 40.25\n",
      "Iteration 12: Average Return = 42.95\n",
      "Iteration 13: Average Return = 42.11\n",
      "Iteration 14: Average Return = 38.86\n",
      "Iteration 15: Average Return = 40.35\n",
      "Iteration 16: Average Return = 42.07\n",
      "Iteration 17: Average Return = 43.94\n",
      "Iteration 18: Average Return = 46.48\n",
      "Iteration 19: Average Return = 46.77\n",
      "Iteration 20: Average Return = 46.67\n",
      "Iteration 21: Average Return = 42.96\n",
      "Iteration 22: Average Return = 45.68\n",
      "Iteration 23: Average Return = 47.96\n",
      "Iteration 24: Average Return = 51.82\n",
      "Iteration 25: Average Return = 53.02\n",
      "Iteration 26: Average Return = 50.82\n",
      "Iteration 27: Average Return = 56.39\n",
      "Iteration 28: Average Return = 51.14\n",
      "Iteration 29: Average Return = 50.7\n",
      "Iteration 30: Average Return = 57.05\n",
      "Iteration 31: Average Return = 54.91\n",
      "Iteration 32: Average Return = 54.38\n",
      "Iteration 33: Average Return = 61.27\n",
      "Iteration 34: Average Return = 56.25\n",
      "Iteration 35: Average Return = 57.03\n",
      "Iteration 36: Average Return = 61.83\n",
      "Iteration 37: Average Return = 62.03\n",
      "Iteration 38: Average Return = 70.21\n",
      "Iteration 39: Average Return = 61.23\n",
      "Iteration 40: Average Return = 68.6\n",
      "Iteration 41: Average Return = 67.79\n",
      "Iteration 42: Average Return = 65.26\n",
      "Iteration 43: Average Return = 66.78\n",
      "Iteration 44: Average Return = 69.88\n",
      "Iteration 45: Average Return = 72.33\n",
      "Iteration 46: Average Return = 75.12\n",
      "Iteration 47: Average Return = 76.62\n",
      "Iteration 48: Average Return = 77.19\n",
      "Iteration 49: Average Return = 83.73\n",
      "Iteration 50: Average Return = 86.27\n",
      "Iteration 51: Average Return = 88.18\n",
      "Iteration 52: Average Return = 91.18\n",
      "Iteration 53: Average Return = 87.27\n",
      "Iteration 54: Average Return = 97.51\n",
      "Iteration 55: Average Return = 108.96\n",
      "Iteration 56: Average Return = 101.76\n",
      "Iteration 57: Average Return = 105.03\n",
      "Iteration 58: Average Return = 113.2\n",
      "Iteration 59: Average Return = 115.36\n",
      "Iteration 60: Average Return = 112.46\n",
      "Iteration 61: Average Return = 115.35\n",
      "Iteration 62: Average Return = 117.41\n",
      "Iteration 63: Average Return = 122.25\n",
      "Iteration 64: Average Return = 118.98\n",
      "Iteration 65: Average Return = 124.3\n",
      "Iteration 66: Average Return = 125.02\n",
      "Iteration 67: Average Return = 126.47\n",
      "Iteration 68: Average Return = 119.22\n",
      "Iteration 69: Average Return = 127.87\n",
      "Iteration 70: Average Return = 120.77\n",
      "Iteration 71: Average Return = 125.55\n",
      "Iteration 72: Average Return = 121.44\n",
      "Iteration 73: Average Return = 125.11\n",
      "Iteration 74: Average Return = 121.35\n",
      "Iteration 75: Average Return = 121.39\n",
      "Iteration 76: Average Return = 119.47\n",
      "Iteration 77: Average Return = 121.08\n",
      "Iteration 78: Average Return = 122.52\n",
      "Iteration 79: Average Return = 118.83\n",
      "Iteration 80: Average Return = 119.3\n",
      "Iteration 81: Average Return = 118.12\n",
      "Iteration 82: Average Return = 119.42\n",
      "Iteration 83: Average Return = 117.23\n",
      "Iteration 84: Average Return = 118.71\n",
      "Iteration 85: Average Return = 122.15\n",
      "Iteration 86: Average Return = 118.71\n",
      "Iteration 87: Average Return = 119.03\n",
      "Iteration 88: Average Return = 118.61\n",
      "Iteration 89: Average Return = 114.82\n",
      "Iteration 90: Average Return = 117.28\n",
      "Iteration 91: Average Return = 116.1\n",
      "Iteration 92: Average Return = 119.56\n",
      "Iteration 93: Average Return = 121.83\n",
      "Iteration 94: Average Return = 121.89\n",
      "Iteration 95: Average Return = 120.26\n",
      "Iteration 96: Average Return = 119.18\n",
      "Iteration 97: Average Return = 121.76\n",
      "Iteration 98: Average Return = 117.37\n",
      "Iteration 99: Average Return = 121.4\n",
      "Iteration 100: Average Return = 114.56\n",
      "Iteration 101: Average Return = 116.79\n",
      "Iteration 102: Average Return = 111.56\n",
      "Iteration 103: Average Return = 116.73\n",
      "Iteration 104: Average Return = 118.38\n",
      "Iteration 105: Average Return = 111.82\n",
      "Iteration 106: Average Return = 113.65\n",
      "Iteration 107: Average Return = 115.32\n",
      "Iteration 108: Average Return = 113.78\n",
      "Iteration 109: Average Return = 109.53\n",
      "Iteration 110: Average Return = 108.81\n",
      "Iteration 111: Average Return = 102.12\n",
      "Iteration 112: Average Return = 105.11\n",
      "Iteration 113: Average Return = 103.99\n",
      "Iteration 114: Average Return = 107.02\n",
      "Iteration 115: Average Return = 107.47\n",
      "Iteration 116: Average Return = 105.51\n",
      "Iteration 117: Average Return = 104.27\n",
      "Iteration 118: Average Return = 99.85\n",
      "Iteration 119: Average Return = 102.73\n",
      "Iteration 120: Average Return = 100.28\n",
      "Iteration 121: Average Return = 101.76\n",
      "Iteration 122: Average Return = 99.19\n",
      "Iteration 123: Average Return = 101.15\n",
      "Iteration 124: Average Return = 107.46\n",
      "Iteration 125: Average Return = 108.67\n",
      "Iteration 126: Average Return = 107.78\n",
      "Iteration 127: Average Return = 106.99\n",
      "Iteration 128: Average Return = 111.83\n",
      "Iteration 129: Average Return = 105.22\n",
      "Iteration 130: Average Return = 110.36\n",
      "Iteration 131: Average Return = 110.46\n",
      "Iteration 132: Average Return = 107.12\n",
      "Iteration 133: Average Return = 114.32\n",
      "Iteration 134: Average Return = 114.24\n",
      "Iteration 135: Average Return = 108.59\n",
      "Iteration 136: Average Return = 110.75\n",
      "Iteration 137: Average Return = 111.76\n",
      "Iteration 138: Average Return = 107.23\n",
      "Iteration 139: Average Return = 110.98\n",
      "Iteration 140: Average Return = 115.31\n",
      "Iteration 141: Average Return = 108.99\n",
      "Iteration 142: Average Return = 115.18\n",
      "Iteration 143: Average Return = 114.37\n",
      "Iteration 144: Average Return = 117.62\n",
      "Iteration 145: Average Return = 123.66\n",
      "Iteration 146: Average Return = 119.68\n",
      "Iteration 147: Average Return = 119.73\n",
      "Iteration 148: Average Return = 121.68\n",
      "Iteration 149: Average Return = 119.6\n",
      "Iteration 150: Average Return = 122.43\n",
      "Iteration 151: Average Return = 116.15\n",
      "Iteration 152: Average Return = 123.5\n",
      "Iteration 153: Average Return = 123.02\n",
      "Iteration 154: Average Return = 119.33\n",
      "Iteration 155: Average Return = 123.01\n",
      "Iteration 156: Average Return = 120.6\n",
      "Iteration 157: Average Return = 128.5\n",
      "Iteration 158: Average Return = 125.2\n",
      "Iteration 159: Average Return = 127.73\n",
      "Iteration 160: Average Return = 127.41\n",
      "Iteration 161: Average Return = 131.92\n",
      "Iteration 162: Average Return = 126.77\n",
      "Iteration 163: Average Return = 126.35\n",
      "Iteration 164: Average Return = 130.26\n",
      "Iteration 165: Average Return = 127.29\n",
      "Iteration 166: Average Return = 130.66\n",
      "Iteration 167: Average Return = 127.98\n",
      "Iteration 168: Average Return = 133.31\n",
      "Iteration 169: Average Return = 131.16\n",
      "Iteration 170: Average Return = 128.31\n",
      "Iteration 171: Average Return = 130.46\n",
      "Iteration 172: Average Return = 126.68\n",
      "Iteration 173: Average Return = 122.05\n",
      "Iteration 174: Average Return = 115.52\n",
      "Iteration 175: Average Return = 126.2\n",
      "Iteration 176: Average Return = 120.93\n",
      "Iteration 177: Average Return = 122.07\n",
      "Iteration 178: Average Return = 124.77\n",
      "Iteration 179: Average Return = 121.69\n",
      "Iteration 180: Average Return = 123.49\n",
      "Iteration 181: Average Return = 121.9\n",
      "Iteration 182: Average Return = 117.74\n",
      "Iteration 183: Average Return = 122.67\n",
      "Iteration 184: Average Return = 123.22\n",
      "Iteration 185: Average Return = 122.06\n",
      "Iteration 186: Average Return = 122.25\n",
      "Iteration 187: Average Return = 124.14\n",
      "Iteration 188: Average Return = 121.5\n",
      "Iteration 189: Average Return = 115.79\n",
      "Iteration 190: Average Return = 119.55\n",
      "Iteration 191: Average Return = 119.73\n",
      "Iteration 192: Average Return = 120.51\n",
      "Iteration 193: Average Return = 118.19\n",
      "Iteration 194: Average Return = 116.48\n",
      "Iteration 195: Average Return = 124.18\n",
      "Iteration 196: Average Return = 124.52\n",
      "Iteration 197: Average Return = 123.51\n",
      "Iteration 198: Average Return = 123.53\n",
      "Iteration 199: Average Return = 123.76\n",
      "Iteration 200: Average Return = 126.07\n",
      "Iteration 201: Average Return = 127.23\n",
      "Iteration 202: Average Return = 125.68\n",
      "Iteration 203: Average Return = 125.64\n",
      "Iteration 204: Average Return = 124.48\n",
      "Iteration 205: Average Return = 122.92\n",
      "Iteration 206: Average Return = 123.73\n",
      "Iteration 207: Average Return = 123.29\n",
      "Iteration 208: Average Return = 124.21\n",
      "Iteration 209: Average Return = 122.22\n",
      "Iteration 210: Average Return = 119.02\n",
      "Iteration 211: Average Return = 125.09\n",
      "Iteration 212: Average Return = 120.37\n",
      "Iteration 213: Average Return = 115.39\n",
      "Iteration 214: Average Return = 109.81\n",
      "Iteration 215: Average Return = 120.1\n",
      "Iteration 216: Average Return = 113.36\n",
      "Iteration 217: Average Return = 113.09\n",
      "Iteration 218: Average Return = 111.38\n",
      "Iteration 219: Average Return = 113.15\n",
      "Iteration 220: Average Return = 111.31\n",
      "Iteration 221: Average Return = 108.84\n",
      "Iteration 222: Average Return = 105.53\n",
      "Iteration 223: Average Return = 102.1\n",
      "Iteration 224: Average Return = 102.45\n",
      "Iteration 225: Average Return = 99.6\n",
      "Iteration 226: Average Return = 97.34\n",
      "Iteration 227: Average Return = 94.63\n",
      "Iteration 228: Average Return = 90.82\n",
      "Iteration 229: Average Return = 96.65\n",
      "Iteration 230: Average Return = 97.59\n",
      "Iteration 231: Average Return = 101.69\n",
      "Iteration 232: Average Return = 103.52\n",
      "Iteration 233: Average Return = 104.28\n",
      "Iteration 234: Average Return = 115.06\n",
      "Iteration 235: Average Return = 116.19\n",
      "Iteration 236: Average Return = 127.59\n",
      "Iteration 237: Average Return = 128.49\n",
      "Iteration 238: Average Return = 138.91\n",
      "Iteration 239: Average Return = 135.45\n",
      "Iteration 240: Average Return = 138.27\n",
      "Iteration 241: Average Return = 148.85\n",
      "Iteration 242: Average Return = 150.89\n",
      "Iteration 243: Average Return = 155.37\n",
      "Iteration 244: Average Return = 163.42\n",
      "Iteration 245: Average Return = 158.68\n",
      "Iteration 246: Average Return = 166.1\n",
      "Iteration 247: Average Return = 159.4\n",
      "Iteration 248: Average Return = 159.71\n",
      "Iteration 249: Average Return = 163.03\n",
      "Iteration 250: Average Return = 163.71\n",
      "Iteration 251: Average Return = 162.44\n",
      "Iteration 252: Average Return = 160.13\n",
      "Iteration 253: Average Return = 165.0\n",
      "Iteration 254: Average Return = 158.34\n",
      "Iteration 255: Average Return = 159.21\n",
      "Iteration 256: Average Return = 155.48\n",
      "Iteration 257: Average Return = 159.31\n",
      "Iteration 258: Average Return = 157.07\n",
      "Iteration 259: Average Return = 161.26\n",
      "Iteration 260: Average Return = 161.02\n",
      "Iteration 261: Average Return = 156.58\n",
      "Iteration 262: Average Return = 162.36\n",
      "Iteration 263: Average Return = 163.42\n",
      "Iteration 264: Average Return = 158.73\n",
      "Iteration 265: Average Return = 160.2\n",
      "Iteration 266: Average Return = 156.31\n",
      "Iteration 267: Average Return = 168.77\n",
      "Iteration 268: Average Return = 170.9\n",
      "Iteration 269: Average Return = 171.44\n",
      "Iteration 270: Average Return = 170.69\n",
      "Iteration 271: Average Return = 168.73\n",
      "Iteration 272: Average Return = 171.38\n",
      "Iteration 273: Average Return = 177.72\n",
      "Iteration 274: Average Return = 175.56\n",
      "Iteration 275: Average Return = 172.04\n",
      "Iteration 276: Average Return = 178.51\n",
      "Iteration 277: Average Return = 177.83\n",
      "Iteration 278: Average Return = 176.97\n",
      "Iteration 279: Average Return = 177.6\n",
      "Iteration 280: Average Return = 173.25\n",
      "Iteration 281: Average Return = 175.19\n",
      "Iteration 282: Average Return = 176.78\n",
      "Iteration 283: Average Return = 178.77\n",
      "Iteration 284: Average Return = 181.92\n",
      "Iteration 285: Average Return = 180.49\n",
      "Iteration 286: Average Return = 181.31\n",
      "Iteration 287: Average Return = 181.51\n",
      "Iteration 288: Average Return = 183.49\n",
      "Iteration 289: Average Return = 174.69\n",
      "Iteration 290: Average Return = 184.19\n",
      "Iteration 291: Average Return = 181.86\n",
      "Iteration 292: Average Return = 178.6\n",
      "Iteration 293: Average Return = 180.47\n",
      "Iteration 294: Average Return = 182.36\n",
      "Iteration 295: Average Return = 177.79\n",
      "Iteration 296: Average Return = 176.42\n",
      "Iteration 297: Average Return = 178.87\n",
      "Iteration 298: Average Return = 177.47\n",
      "Iteration 299: Average Return = 173.54\n",
      "Iteration 300: Average Return = 173.13\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unbound method close() must be called with Session instance as first argument (got nothing instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-acf6dc042891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mHistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIteration_to_complete\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unbound method close() must be called with Session instance as first argument (got nothing instead)"
     ]
    }
   ],
   "source": [
    "history = collections.namedtuple(\"History\", [\"Iteration_to_complete\"])\n",
    "History = history(Iteration_to_complete = np.zeros(10))\n",
    "for i in range(10):\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Construct a neural network to represent policy which maps observed state to action. \n",
    "    in_dim = util.flatten_space(env.observation_space)\n",
    "    out_dim = util.flatten_space(env.action_space)\n",
    "    hidden_dim = 8\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    policy = CategoricalPolicy(in_dim, out_dim, hidden_dim, opt, sess)\n",
    "\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    n_iter = 300\n",
    "    n_episode = 100\n",
    "    path_length = 200\n",
    "    discount_rate = 0.99\n",
    "    baseline = LinearFeatureBaseline(env.spec)\n",
    "\n",
    "    po = PolicyOptimizer(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                         discount_rate)\n",
    "    \n",
    "    Iteration = po.train()\n",
    "    \n",
    "    History.Iteration_to_complete[i] = Iteration\n",
    "    tf.Session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter = 300\n",
    "n_episode = 100\n",
    "path_length = 200\n",
    "discount_rate = 0.99\n",
    "baseline = LinearFeatureBaseline(env.spec)\n",
    "\n",
    "po = PolicyOptimizer(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                     discount_rate)\n",
    "\n",
    "# Train the policy optimizer\n",
    "po.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify your solutions\n",
    "\n",
    "if you solve the problems 1~4 correctly, your will solve CartPole with roughly ~ 80 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "Replacing line \n",
    "\n",
    "`baseline = LinearFeatureBaseline(env.spec)` \n",
    "\n",
    "with \n",
    "\n",
    "`baseline = None`\n",
    "\n",
    "can remove the baseline.\n",
    "\n",
    "Modify the code to compare the variance and performance before and after adding baseline.\n",
    "Then, write a report about your findings. (with figures is better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "\n",
    "In function process_paths of class `PolicyOptimizer`, why we need to normalize the advantages? i.e., what's the usage of this line:\n",
    "\n",
    "`p[\"advantages\"] = (a - a.mean()) / (a.std() + 1e-8)`\n",
    "\n",
    "Include the answer in your report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
